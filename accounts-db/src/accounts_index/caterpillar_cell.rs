#![allow(dead_code)]
#![allow(unused_variables)]
use {
    modular_bitfield::{bitfield, prelude::*},
    static_assertions::const_assert_eq,
    std::{
        fmt::Debug,
        marker::PhantomData,
        ptr::NonNull,
        sync::atomic::{AtomicU64, Ordering},
    },
};

/// Represents a compact payload that can be embedded within a `CaterpillarCell`.
///
/// This trait marks types that can be stored directly within the 64-bit atomic value
/// of a `CaterpillarCell`. The payload is limited to 61 bits though to leave 3 bits for
/// metadata.
///
/// The trait is automatically implemented for `module_bitfield::Specifier` generated by
/// #[bitfield(bits = 61)] annotation such that the struct fits in 64 bits, but actual payload
/// spans only 61 bits.
///
/// # Example
/// ```
/// #[bitfield(bits = 61)]
/// #[derive(BitfieldSpecifier)]
/// struct MyPayload {
///     id: B32,
///     flags: B29,
/// }
/// ```
pub trait CompactPayload {
    const COMPACT_BITS: usize = 61;

    fn from_raw(raw: u64) -> Self;
    fn into_raw(self) -> u64;
}

impl<T: Specifier<Bytes = u64, InOut = T>> CompactPayload for T {
    fn from_raw(raw: u64) -> Self {
        debug_assert_eq!(Self::BITS, Self::COMPACT_BITS);
        T::from_bytes(raw).expect("only 61 least significant bits can be set")
    }

    fn into_raw(self) -> u64 {
        T::into_bytes(self).unwrap()
    }
}

pub trait ExpandedPayload<C: CompactPayload> {
    fn from_compact(compact: C) -> Self;
    fn into_compact(self) -> C;
}

/// A thread-safe container that can dynamically switch between compact and expanded payloads.
///
/// `CaterpillarCell` starts with a compact payload `C` embedded directly in a 64-bit atomic value.
/// When more space is needed, it can atomically transition to an expanded payload `E` stored on
/// the heap. This design optimizes for the common case where data remains small while supporting
/// efficient growth when needed.
///
/// # Type Parameters
/// - `C`: Compact payload type that implements `CompactPayload` (usable payload â‰¤ 61 bits)
/// - `E`: Expanded payload type (aligned to 8-bytes) stored on the heap when data grows
///
/// # Thread Safety
/// All operations on `CaterpillarCell` are atomic and thread-safe. Multiple threads can
/// simultaneously call `turn_to_expanded()` on the `&` of the same cell - only one will succeed
/// in the transition, while others will observe the already-expanded state.
///
/// Transitioning back from expanded to compact representation currently requires `&mut` access
/// to the cell.
///
/// # Example
/// ```
/// let cell = CaterpillarCell::from_compact(small_payload);
///
/// // Multiple threads can safely expand concurrently
/// match cell.turn_to_expanded() {
///     CaterpillarCellView::Expanded(expanded) => {
///         // Now can use the & to expanded data (requires inner lock for concurrent updates)
///     }
///     _ => unreachable!(),
/// }
/// ```
#[derive(Debug, Default)]
pub struct CaterpillarCell<C: CompactPayload, E> {
    /// Raw value - either an embedded (small) payload or address of a boxed full value
    raw_atomic: AtomicU64,
    _phantom: PhantomData<(C, E)>,
}

impl<C, E> CaterpillarCell<C, E>
where
    C: CompactPayload,
    E: ExpandedPayload<C>,
{
    /// Creates a new `CaterpillarCell` containing a compact payload.
    pub fn from_compact(compact: C) -> Self {
        Self::from_transumtion_cell(TransmutationCell::from_compact(compact))
    }

    /// Creates a new `CaterpillarCell` containing an expanded payload.
    pub fn from_expanded(expanded: E) -> Self {
        Self::from_transumtion_cell(TransmutationCell::from_expanded(expanded))
    }

    fn from_transumtion_cell(transmution_cell: TransmutationCell<C, E>) -> Self {
        Self {
            raw_atomic: AtomicU64::new(transmution_cell.raw()),
            _phantom: PhantomData,
        }
    }

    /// Returns a view of the current state without modifying it.
    ///
    /// This method provides read-only access to the cell's contents, whether
    /// it's currently in compact or expanded state. The operation is atomic
    /// and does not require exclusive access.
    pub fn as_view(&self) -> CaterpillarCellView<C, E> {
        TransmutationCell::from_raw(self.raw_atomic.load(Ordering::Relaxed)).into_view()
    }

    /// Atomically ensures the cell is in expanded state and returns a view.
    ///
    /// If the cell is currently compact, this method will atomically transition
    /// it to the expanded state by converting the compact payload to an expanded
    /// payload and storing it on the heap. If it's already expanded, it returns
    /// the existing expanded view.
    ///
    /// # Thread Safety
    /// This operation is fully thread-safe using compare-and-swap. When multiple
    /// threads call this method concurrently on a compact cell:
    /// - Only one thread will succeed in the transition (first wins)
    /// - Other threads will see the already-expanded state
    /// - All threads can then safely access the expanded payload
    ///
    /// # Returns
    /// Always returns `CaterpillarCellView::Expanded` containing a reference to
    /// the expanded data, either newly created or pre-existing.
    pub fn turn_to_expanded(&self) -> CaterpillarCellView<C, E> {
        let mut current_raw = self.raw_atomic.load(Ordering::Acquire);
        loop {
            let current_holder = TransmutationCell::from_raw(current_raw);
            match current_holder.into_view() {
                CaterpillarCellView::Compact(compact) => {
                    let new_holder = TransmutationCell::from_expanded(E::from_compact(compact));
                    match self.raw_atomic.compare_exchange(
                        current_raw,
                        new_holder.raw(),
                        Ordering::SeqCst,
                        Ordering::SeqCst,
                    ) {
                        Ok(_) => return new_holder.into_view(),
                        Err(different_raw) => {
                            // entry that we created and tried to store will not be used, so deallocate it
                            new_holder.deallocate_box_if_expanded();
                            // continue loop to re-check if the entry is still compact
                            current_raw = different_raw
                        }
                    }
                }
                e @ CaterpillarCellView::Expanded(_) => return e,
            }
        }
    }

    /// Converts an expanded cell back to compact state.
    ///
    /// This method requires a mutable reference, ensuring exclusive access to the cell.
    /// If the cell is currently expanded, it will deallocate the heap data and convert
    /// it back to a compact representation embedded in the atomic value. If the cell
    /// is already compact, it returns the current compact view.
    ///
    /// # Returns
    /// Always returns `CaterpillarCellView::Compact` containing the compact data,
    /// either newly converted or pre-existing.
    pub fn turn_to_compact(&mut self) -> CaterpillarCellView<C, E> {
        let current_raw = self.raw_atomic.load(Ordering::Acquire);
        let current_transmute = TransmutationCell::from_raw(current_raw);
        if current_transmute.is_compact() {
            return current_transmute.into_view();
        }
        let expanded = current_transmute
            .deallocate_box_if_expanded()
            .expect("entry should be of expanded kind");
        let new_transmute = TransmutationCell::from_compact(expanded.into_compact());
        // We have exclusive (&mut) access to the entry, so no other code is reading or updating
        // the raw atomic. This means we can just store new value without conflict.
        self.raw_atomic
            .store(new_transmute.raw(), Ordering::Release);
        new_transmute.into_view()
    }
}

impl<C: CompactPayload, E> Drop for CaterpillarCell<C, E> {
    fn drop(&mut self) {
        let previous_raw = self.raw_atomic.swap(0, Ordering::AcqRel);
        if previous_raw != 0 {
            TransmutationCell::<C, E>::from_raw(previous_raw).deallocate_box_if_expanded();
        }
    }
}

/// A view into the current state of a `CaterpillarCell`.
///
/// This enum provides access to the cell's payload regardless of whether it's
/// currently stored in compact or expanded form. The lifetime `'a` ties the
/// view to the cell that created it.
#[derive(Debug)]
pub enum CaterpillarCellView<'a, C, E> {
    /// The payload is stored compactly within the cell itself.
    Compact(C),
    /// The payload is stored on the heap, referenced by this view.
    Expanded(&'a E),
}

/// Metadata stored in the low 3 bits of the cell's atomic value.
///
/// This structure contains flags that indicate how to interpret the remaining
/// 61 bits of the atomic value - either as embedded compact data or as a
/// shifted heap pointer.
#[bitfield(bits = 3)]
#[repr(C)]
#[derive(Debug, Default, Copy, Clone, Eq, PartialEq, BitfieldSpecifier)]
pub struct CommonPayloadInfo {
    /// Whether the payload is compact in the entry itself (otherwise entry stores address to heap memory)
    pub is_compact: bool,
    /// Reserved bits for future use
    pub reserved: B2,
}

/// The complete 64-bit representation combining metadata and payload data.
#[bitfield(bits = 64)]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct CommonInfoAndPayload {
    /// 3-bit metadata indicating payload type and flags
    pub common_info: CommonPayloadInfo,
    /// 61-bit payload representation (either embedded data or shifted pointer)
    pub payload_repr: B61,
}

#[repr(C)]
union TransmutationCell<C, E> {
    raw: u64,
    structured: CommonInfoAndPayload,
    _phantom: PhantomData<(C, E)>,
}

impl<C: CompactPayload, E> TransmutationCell<C, E> {
    const ALIGNED_PTR_SHIFT: u64 = CommonPayloadInfo::BITS as u64;

    fn from_raw(raw: u64) -> Self {
        debug_assert_eq!(size_of::<Self>(), size_of::<u64>());
        assert_ne!(
            raw, 0,
            "0 is not a valid value, i.e. neither it is compact nor holds a valid heap address"
        );
        Self { raw }
    }

    fn from_compact(compact: C) -> Self {
        debug_assert_eq!(
            C::COMPACT_BITS + CommonPayloadInfo::BITS,
            <u64 as Specifier>::BITS
        );
        let embedded_payload = C::into_raw(compact);
        let structured = CommonInfoAndPayload::new()
            .with_common_info(CommonPayloadInfo::new().with_is_compact(true))
            .with_payload_repr(embedded_payload);
        const_assert_eq!(size_of::<CommonInfoAndPayload>(), size_of::<u64>());
        Self { structured }
    }

    fn from_expanded(expanded: E) -> Self {
        debug_assert_eq!(align_of::<E>(), align_of::<u64>());
        let expanded_ptr = NonNull::new(Box::into_raw(Box::new(expanded))).unwrap();
        let shifted_address = Self::make_ptr_shifted_address(expanded_ptr);
        let structured = CommonInfoAndPayload::new()
            .with_common_info(CommonPayloadInfo::new().with_is_compact(false))
            .with_payload_repr(shifted_address);
        Self { structured }
    }

    fn raw(&self) -> u64 {
        // Getting full raw representation of the cell is always safe
        unsafe { self.raw }
    }

    fn is_compact(&self) -> bool {
        unsafe { self.structured.common_info().is_compact() }
    }

    fn into_view<'a>(self) -> CaterpillarCellView<'a, C, E> {
        unsafe {
            if self.is_compact() {
                CaterpillarCellView::Compact(C::from_raw(self.structured.payload_repr()))
            } else {
                let ptr = self.get_expanded_ptr();
                CaterpillarCellView::Expanded(ptr.as_ref().unwrap())
            }
        }
    }

    /// If entry is expanded, converts the saved heap address to the entry itself
    ///
    /// Note: this will deallocate the entry from the heap
    fn deallocate_box_if_expanded(self) -> Option<E> {
        (!self.is_compact()).then(|| unsafe {
            let ptr = self.get_expanded_mut_ptr();
            *Box::from_raw(ptr)
        })
    }

    /// Return a down-shifted (by 3 bits) memory address of a non-null `E` pointer
    ///
    /// `E` must be a 8-byte aligned struct
    fn make_ptr_shifted_address(ptr: NonNull<E>) -> u64 {
        let address = ptr.addr().get() as u64;
        assert_eq!(address & ((1 << Self::ALIGNED_PTR_SHIFT) - 1), 0);
        address >> Self::ALIGNED_PTR_SHIFT
    }

    unsafe fn get_expanded_ptr(&self) -> *const E {
        unsafe {
            let address = self.structured.payload_repr() << Self::ALIGNED_PTR_SHIFT;
            address as *const E
        }
    }

    unsafe fn get_expanded_mut_ptr(&self) -> *mut E {
        unsafe {
            let address = self.structured.payload_repr() << Self::ALIGNED_PTR_SHIFT;
            address as *mut E
        }
    }
}

#[cfg(test)]
mod tests {
    use {
        super::*,
        std::{
            sync::{Arc, Barrier, RwLock},
            thread,
        },
    };

    #[derive(Debug)]
    pub struct TestExpandedPayload {
        pub beeps: u32,
        pub inner_payloads: RwLock<Vec<TestCompactPayload>>,
    }

    #[bitfield(bits = 61)]
    #[repr(C)]
    #[derive(Clone, Debug, BitfieldSpecifier)]
    pub struct TestCompactPayload {
        pub id: B29,
        pub offset: B31,
        pub is_bar: bool,
    }

    impl ExpandedPayload<TestCompactPayload> for TestExpandedPayload {
        fn from_compact(compact: TestCompactPayload) -> Self {
            Self {
                beeps: 1,
                inner_payloads: RwLock::new(vec![compact]),
            }
        }

        fn into_compact(self) -> TestCompactPayload {
            let payloads = self.inner_payloads.into_inner().unwrap();
            TestCompactPayload::new()
                .with_id(payloads[0].id())
                .with_offset(payloads[0].offset())
        }
    }

    type TestCaterpillarCell = CaterpillarCell<TestCompactPayload, TestExpandedPayload>;

    #[test]
    fn test_cell_turning() {
        let compact = TestCompactPayload::new()
            .with_id(4534)
            .with_offset(99)
            .with_is_bar(true);
        assert_eq!(
            TestCompactPayload::from_bytes(compact.clone().into_bytes()).id(),
            4534
        );

        let compact_cell = TestCaterpillarCell::from_compact(compact);
        let CaterpillarCellView::Compact(compact) = compact_cell.as_view() else {
            panic!("Unexpected entry type")
        };
        assert_eq!(compact.id(), 4534);
        assert_eq!(compact.offset(), 99);

        let expanded = TestExpandedPayload {
            beeps: 2,
            inner_payloads: RwLock::new(vec![compact]),
        };
        let expanded_cell = TestCaterpillarCell::from_expanded(expanded);
        let CaterpillarCellView::Expanded(expanded) = expanded_cell.as_view() else {
            panic!("Unexpected entry type")
        };
        assert_eq!(expanded.beeps, 2);
        assert_eq!(expanded.inner_payloads.read().unwrap()[0].id(), 4534);
        assert_eq!(expanded.inner_payloads.read().unwrap()[0].offset(), 99);

        let turning_cell = compact_cell;
        let CaterpillarCellView::Expanded(expanded) = turning_cell.turn_to_expanded() else {
            panic!("Unexpected entry type")
        };
        assert_eq!(expanded.beeps, 1);
        assert_eq!(expanded.inner_payloads.read().unwrap()[0].id(), 4534);
        assert_eq!(expanded.inner_payloads.read().unwrap()[0].offset(), 99);

        let mut mut_turning_cell = turning_cell;
        mut_turning_cell.turn_to_compact();
        let CaterpillarCellView::Compact(compact) = mut_turning_cell.as_view() else {
            panic!("Unexpected entry type")
        };
        assert_eq!(compact.id(), 4534);
        assert_eq!(compact.offset(), 99);
    }

    #[test]
    fn test_concurrent_expand() {
        const NUM_THREADS: usize = 5;
        const INITIAL_FILE_ID: u32 = 1000;

        // Create an initial compact entry
        let initial_compact = TestCompactPayload::new()
            .with_id(INITIAL_FILE_ID)
            .with_offset(100)
            .with_is_bar(false);

        let cell = Arc::new(TestCaterpillarCell::from_compact(initial_compact));

        // Create barrier to synchronize thread starts
        let barrier = Arc::new(Barrier::new(NUM_THREADS));

        // Spawn N threads, each adding an entry with unique file_id
        let handles: Vec<_> = (0..NUM_THREADS)
            .map(|thread_id| {
                let cell_clone = Arc::clone(&cell);
                let barrier_clone = Arc::clone(&barrier);

                thread::spawn(move || {
                    barrier_clone.wait(); // Synchronize start

                    // All threads race to call turn_to_expanded
                    let view = cell_clone.turn_to_expanded();

                    match view {
                        CaterpillarCellView::Expanded(expanded) => {
                            // Create new entry with unique file_id for this thread
                            let unique_file_id = INITIAL_FILE_ID + 1000 + (thread_id as u32);
                            // Add entry to slot_list under write lock
                            expanded.inner_payloads.write().unwrap().push(
                                TestCompactPayload::new()
                                    .with_id(unique_file_id)
                                    .with_offset(50)
                                    .with_is_bar(true),
                            );

                            unique_file_id // Return the file_id this thread added
                        }
                        _ => panic!("Expected expanded entry after turn_to_expanded"),
                    }
                })
            })
            .collect();

        // Wait for all threads to complete and collect their file_ids
        let added_file_ids: Vec<u32> = handles
            .into_iter()
            .map(|handle| handle.join().unwrap())
            .chain([INITIAL_FILE_ID])
            .collect();

        let CaterpillarCellView::Expanded(expanded) = cell.as_view() else {
            panic!("Expected expanded entry at end");
        };
        let slot_list = expanded.inner_payloads.read().unwrap();

        // Should have original entry + N added by threads
        assert_eq!(slot_list.len(), NUM_THREADS + 1);

        // Verify each thread's entry was added
        for (thread_id, &expected_file_id) in added_file_ids.iter().enumerate() {
            let thread_entry = slot_list
                .iter()
                .find(|e| e.id() == expected_file_id)
                .unwrap_or_else(|| panic!("Missing entry for thread {thread_id}"));

            assert_eq!(
                thread_entry.offset(),
                50 * (1 + thread_id / NUM_THREADS) as u32
            );
            assert_eq!(thread_entry.is_bar(), thread_id < NUM_THREADS);
        }

        assert_eq!(expanded.beeps, 1);
    }
}
